# NLP with Disaster Tweets — Mini‑Project

**Author:** Anees Shaikh  
**Date:** 2025-09-17 15:21

## Overview
This repository contains my course mini‑project for the Kaggle competition: **Natural Language Processing with Disaster Tweets**.  
The goal is to classify tweets as *disaster‑related (1)* or *not (0)*. The competition is evaluated using **F1 score**.

## Contents
- `NLP_Disaster_Tweets_Project.ipynb` — full notebook (EDA → modeling → submission)
- `submission.csv` — latest submission file (generated by the notebook)
- `img/leaderboard.png` — *(add)* screenshot of my leaderboard position
- `requirements.txt` — *(optional)* libraries used
- `LICENSE` — *(optional)*

## How to Reproduce (Kaggle)
1. Open the notebook in Kaggle.
2. Add **Input Datasets**:
   - `nlp-getting-started` (competition data)
   - *(optional)* a dataset providing `glove.6B.100d.txt` for pretrained embeddings
3. Run all cells to generate `submission.csv`.
4. Click **"Submit to Competition"** in the Outputs panel.
5. Publish the notebook and take a **leaderboard screenshot** after submission.

## Notes
- I compare **TF‑IDF + Logistic Regression** vs. **BiLSTM/GRU** and pick the best by validation F1.
- Cleaning is intentionally lightweight to preserve signal in short tweets.
- Future work: try transformers (e.g., DistilBERT/RoBERTa) to push F1 higher.

## References
- Kaggle Competition: Natural Language Processing with Disaster Tweets
- scikit‑learn (TF‑IDF, Logistic Regression) docs
- Keras (LSTM/GRU) docs
- GloVe: Pennington, Socher, Manning (2014)
